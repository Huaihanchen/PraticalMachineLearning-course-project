---
title: "Pratical Machine Learning Course Project"
author: "Huaihan Chen"
date: "Apirl 16, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ¨C a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Load packages

Load the packages that may be used in the code.
```{r message=FALSE,warning=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
```

## Download Data from the website


```{r}
url1<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml_training.csv"
testFile  <- "./data/pml_testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(url1, destfile=trainFile)
}
if (!file.exists(testFile)) {
  download.file(url2, destfile=testFile)
}
```

## Read data

Now we will read the two csv files into R.

```{r}
train_raw<-read.csv("./data/pml_training.csv")
test_raw<-read.csv("./data/pml_testing.csv")
```

To better understand the datasets, we use dim() and str() function to simply take a look at the data sets.

```{r results='hide'}
dim(train_raw)
dim(test_raw)
str(train_raw)
str(test_raw)
```

The training data set contains 19622 observations and 160 variables, while the testing data set contains 20 observations and 160 variables. However, both data sets contain some NA's, and there are some variables that carry little information--that is to say, some variables contribute little to the measurements. So, we should first clean the data.

## Clean the data

In this section, we will remove all the NA's, as well as the "useless" variables in the training and testing data sets

```{r}
sum(complete.cases(train_raw))
```

First, remove the NA's

```{r}
train_raw <- train_raw[, colSums(is.na(train_raw)) == 0] 
test_raw <- test_raw[, colSums(is.na(test_raw)) == 0]
```

Next, remove the "useless" variables

```{r}
classe <- train_raw$classe
train_remove <- grepl("^X|timestamp|window", names(train_raw))
train_raw <- train_raw[, !train_remove]
train_cleaned <- train_raw[, sapply(train_raw, is.numeric)]
train_cleaned$classe <- classe
test_remove <- grepl("^X|timestamp|window", names(test_raw))
test_raw <- test_raw[, !test_remove]
test_cleaned <- test_raw[, sapply(test_raw, is.numeric)]
```

Now, the cleaned data sets contains only useful columns and the "classe" variable is still in the data sets

## Slice the data sets

In this section, we will split the original training set into a pure training set (70%) and a validation set (30%).

```{r}
set.seed(2333)   # reproducible
inTrain <- createDataPartition(train_cleaned$classe, p=0.70, list=F)
train_data <- train_cleaned[inTrain, ]
test_data <- train_cleaned[-inTrain, ]
```

## Modeling 

We fit a predictive model with Random Forest algorithm, since it automatically selects important variables and is robust to correlated covariates & outliers in general. We will use 6-fold cross validation when applying the algorithm.

```{r}
control_rf <- trainControl(method="cv", 6)
model_rf <- train(classe ~ ., data=train_data, method="rf", trControl=control_rf, ntree=250)
```

Then, we estimate the performance of the model on the validation set.

```{r}
predict_rf <- predict(model_rf, test_data)
confusionMatrix(test_data$classe, predict_rf)
```

## Evaluate the out-of-sample error

We use the out-of-sample error to be our estimation of our model.

```{r}
accuracy <- postResample(predict_rf, test_data$classe)
accuracy
oose <- 1 - as.numeric(confusionMatrix(test_data$classe, predict_rf)$overall[1])
oose
```

So, the estimated accuracy of the model is 99.51%, and the out-of-sample error is 0.49%. As a consequence, our model perform well.

## Predicting

Now, we apply our model to the original testing set (with the problem_id removed). 

```{r}
pred <- predict(model_rf, test_cleaned[, -length(names(test_cleaned))])  # remove problem_id
pred
```